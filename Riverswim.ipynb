{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# index indicates where the node is in the river. index = 0 is equivalent to left bank and index = L is equivalent to right bank\n",
    "# rewards and transition_probs should be dicts of the form\n",
    "# rewards = {\"L\" : r_L, \"R\" : r_R} or L replaced w. R for indicating going right\n",
    "# transitions_probs = {\"L\": np.array([p^L_{index-1}, p^L_{index}, p^L_{index+1}],\n",
    "#                      \"R\": np.array([p^R_{index-1}, p^R_{index}, p^R_{index+1}]}\n",
    "# Where p^A_{i} gives the transition probability from the current state to state i under action A. We can input only 3 values instead of transition matrix\n",
    "# As all other probabilities in River-Swim are zer\n",
    "\n",
    "\n",
    "class RiverNode:\n",
    "    def __init__(self, rewards, transition_probs):\n",
    "        #Set rewards and transition probabilities for a single node\n",
    "        self.rewards = rewards\n",
    "        self.transition_probs = transition_probs\n",
    "        self.times_visited = 0\n",
    "        self.actions_taken = {\"L\": 0 ,\"R\": 0}\n",
    "        self.emperical_reward = {\"L\":0, \"R\":0}\n",
    "        self.emperical_transitions = {\"L\":{\"-1\":0, \"0\":0, \"1\":0}, \"R\":{\"-1\":0, \"0\":0, \"1\":0}}\n",
    "        self.times_since_last_play = {\"L\":0, \"R\":0}\n",
    "\n",
    "    # Visits node and returns transition as well as reward\n",
    "    def visit(self, action):\n",
    "        # Update parameters of node \n",
    "        self.times_visited +=  1\n",
    "        self.actions_taken[action] += 1\n",
    "        self.emperical_reward[action] += self.rewards[action]\n",
    "        # Sample next state and update transition \n",
    "        sample = np.random.choice([-1,0,1], p = self.transition_probs[action])\n",
    "        self.emperical_transitions[action][str(sample)] += 1\n",
    "        for play in [\"R\",\"L\"]:\n",
    "            if action != play:\n",
    "                self.times_since_last_play[play] +=1\n",
    "            else:\n",
    "                self.times_since_last_play[play] = 0\n",
    "\n",
    "        return sample,self.rewards[action]\n",
    "\n",
    "    def get_estimate(self):\n",
    "        reward_estimate = {}\n",
    "        transition_estimate = {}\n",
    "        for action in [\"L\",\"R\"]:\n",
    "            reward_estimate[action] = self.emperical_reward[action]/max(1,self.actions_taken[action])\n",
    "            transition_estimate[action] = np.array([self.emperical_transitions[action][next_s]/max(self.actions_taken[action],1) for next_s in [\"-1\",\"0\",\"1\"]])\n",
    "        return RiverNode(rewards=reward_estimate, transition_probs=transition_estimate)\n",
    "    def reset_node(self):\n",
    "        #Set rewards and transition probabilities for a single node\n",
    "        self.times_visited = 0\n",
    "        self.actions_taken = {\"L\": 0 ,\"R\": 0}\n",
    "        self.emperical_reward = {\"L\":0, \"R\":0}\n",
    "        self.emperical_transitions = {\"L\":{\"-1\":0, \"0\":0, \"1\":0}, \"R\":{\"-1\":0, \"0\":0, \"1\":0}}\n",
    "#Make a class generating the environment. Takes arguments:\n",
    "#n_states = #states in the riwerswim environment\n",
    "# The following parameters are used to construct instances of RiverNode class, and should be passed as described above\n",
    "\n",
    "#reward_leftbank = The reward for choosing left at index = 0\n",
    "#reward_rightbank = The reward for choosing right at index = n_states\n",
    "#reward_other = reward for other nodes (in-between terminal)\n",
    "\n",
    "\n",
    "#trans_rightbank = transition probabilities in the rightmost bank\n",
    "#trans_leftbank = transition probabilities in the leftmost bank\n",
    "#trans_other = transition probabilities for other nodes (in-between terminal)\n",
    "\n",
    "\n",
    "class RiverSwimEnvironment:\n",
    "    def __init__(self, gamma, nodes = None, n_states = None, reward_leftbank ={\"L\":0.05, \"R\":0}, reward_other = {\"L\":0, \"R\":0}, reward_rightbank = {\"L\":0, \"R\":1},\n",
    "    trans_leftbank = {\"L\":np.array([0,1,0]),\"R\":np.array([0,0.6,0.4])}, trans_other = {\"L\":np.array([1,0,0]),\"R\":np.array([0.05,0.55,0.4])},\n",
    "    trans_rightbank = {\"L\":np.array([1,0,0]),\"R\":np.array([0.4,0.6,0])}):\n",
    "        self.nodes = nodes\n",
    "        if nodes is None:\n",
    "            left_node = RiverNode(rewards=reward_leftbank, transition_probs=trans_leftbank) # Generate left bank node\n",
    "            right_node = RiverNode(rewards=reward_rightbank, transition_probs=trans_rightbank) # generate right bank node\n",
    "            self.nodes = [left_node] + ([RiverNode(rewards=reward_other, transition_probs=trans_other) for i in range(n_states-2)]) + [right_node] # Make a list of nodes\n",
    "        self.n_states = len(self.nodes)\n",
    "        self.gamma = gamma # set discount\n",
    "    \n",
    "    def reset_all(self):\n",
    "        for node in self.nodes:\n",
    "            node.reset_node()\n",
    "\n",
    "    #Method for constructing reward-vector, transition-matrix and Value-vector given a policy.\n",
    "    \n",
    "    # Policy should be of the form np.array([\"L\",\"R\",\"L\"...]) such that policy[i] = action taken in state i\n",
    "    # Mostly legacy for constructing all_L and all_R\n",
    "\n",
    "    def gen_matrices(self, policy):\n",
    "        reward_vector = np.array([self.nodes[i].rewards[policy[i]] for i in range(self.n_states)]) # Generates \n",
    "\n",
    "        transitionmatrix = np.zeros((self.n_states+1, self.n_states+1)) # Initialise 0-matrix for transition probabilities. Pad edges with +1\n",
    "        \n",
    "        for i in range(self.n_states):\n",
    "            transitionmatrix[i, [i-1,i,i+1]] = self.nodes[i].transition_probs[policy[i]] # Fill out transition_matrix\n",
    "        \n",
    "        transitionmatrix = np.array(transitionmatrix)[0:self.n_states,0:self.n_states] # Make a numpy-array and throw padding away\n",
    "        \n",
    "        value_vector = np.linalg.inv(np.identity(self.n_states)-self.gamma*transitionmatrix) @ reward_vector # Calculate value of policy\n",
    "\n",
    "        return {\"reward\":reward_vector, \"transition\":transitionmatrix, \"value\":value_vector} # Output dict containing reward vektor, transition matrix and value vector\n",
    "    # Generates transition/reward of all_L and all_R policies. All other (stationary) policies can be derived from taking a combination of these\n",
    "    def gen_all(self):\n",
    "        self.all_R = self.gen_matrices(np.repeat(\"R\", self.n_states))\n",
    "        self.all_L = self.gen_matrices(np.repeat(\"L\", self.n_states))\n",
    "\n",
    "    #Does the same as gen_matrices, but for stochastic policies. Should be called after gen_all()\n",
    "    def eval_stochastic_policy(self, p_go_right=np.array([0.5,0.5,0.5,0.5])):\n",
    "        reward_vector = self.all_R[\"reward\"] * p_go_right + self.all_L[\"reward\"]*(1-p_go_right)\n",
    "        transition_matrix = self.all_R[\"transition\"]*p_go_right + self.all_L[\"transition\"]*(1-p_go_right)\n",
    "        value_vector = np.linalg.inv(np.identity(self.n_states)-self.gamma*transition_matrix) @ reward_vector # Calculate value of policy\n",
    "        return {\"reward\":reward_vector, \"transition\":transition_matrix, \"value\":value_vector} # Output dict containing reward vektor, transition matrix and value vector\n",
    "\n",
    "    \n",
    "    def value_iteration(self,epsilon):\n",
    "        n = 0 # set n = 0\n",
    "        rmax = max([max(self.nodes[i].rewards.values()) for i in range(self.n_states)]) # Get rmax\n",
    "        V_n = np.zeros(self.n_states) # Set V_n to 0 (arbitrary)\n",
    "        V_np1  = np.ones(self.n_states)*rmax*(1/(1-self.gamma)) # Set V_1 = rmax/(1-gamma)*(1-vector)\n",
    "        \n",
    "        #Generate reward/transition-matrices\n",
    "        all_R = self.gen_matrices(np.repeat(\"R\", self.n_states))\n",
    "        all_L = self.gen_matrices(np.repeat(\"L\", self.n_states))\n",
    "\n",
    "        while(np.max(np.abs(V_np1-V_n))>= (1-self.gamma)/(2*self.gamma)*epsilon):\n",
    "            V_n = V_np1\n",
    "            #Compute V_{n+1}(s) = max (r(s,a) + gamma*sum(p(x|s,a)*V_{n}(x))) for each s, by doing creating transition/reward-matrices for \"L\", \"R\"-actions in each state.\n",
    "            V_np1_R = all_R[\"reward\"] + self.gamma*all_R[\"transition\"]@V_np1\n",
    "            V_np1_L = all_L[\"reward\"] + self.gamma*all_L[\"transition\"]@V_np1\n",
    "            V_np1 = np.maximum(V_np1_R,V_np1_L)\n",
    "            n=n+1\n",
    "        final_policy = np.where(V_np1_R == V_np1, \"R\",\"L\") # Get final policy\n",
    "        self.n_iter_VI = n\n",
    "        self.final_policy_VI = final_policy\n",
    "        self.final_valuef_VI = V_np1\n",
    "\n",
    "    def policy_iteration(self):\n",
    "        n = 0\n",
    "        #pi_n =   np.random.choice(np.array([\"L\",\"R\"]), self.n_states) # Initialise policies as random\n",
    "        #pi_np1 = np.random.choice(np.array([\"L\",\"R\"]), self.n_states)\n",
    "        \n",
    "        #Initialize PI w. speedup\n",
    "        pi_n = np.repeat(\"R\", self.n_states)\n",
    "        pi_n[0] = \"L\"\n",
    "        pi_np1 = np.repeat(\"R\", self.n_states)\n",
    "\n",
    "        if np.all(pi_n == pi_np1):\n",
    "            self.policy_iteration()\n",
    "\n",
    "        all_R = self.gen_matrices(np.repeat(\"R\", self.n_states))\n",
    "        all_L = self.gen_matrices(np.repeat(\"L\", self.n_states))\n",
    "\n",
    "        while np.any(pi_n != pi_np1):\n",
    "            pi_n = pi_np1\n",
    "            V_n = self.gen_matrices(pi_n)[\"value\"] #Find value of pi_n\n",
    "            # Generate rewards for every action in each state\n",
    "\n",
    "            V_np1_R = all_R[\"reward\"] + self.gamma*all_R[\"transition\"]@V_n\n",
    "            V_np1_L = all_L[\"reward\"] + self.gamma*all_L[\"transition\"]@V_n\n",
    "            V_np1 = np.maximum(V_np1_R,V_np1_L) # Get elementwise max\n",
    "            pi_np1 = np.where(V_np1_R == V_np1, \"R\",\"L\") # Get argmax\n",
    "            n = n+1\n",
    "\n",
    "        self.n_iter_PI = n\n",
    "        self.final_policy_PI = pi_np1\n",
    "        self.final_valuef_PI = V_np1\n",
    "        Q_star = np.empty((self.n_states,2))\n",
    "        Q_star[:,0] = all_L[\"reward\"] + self.gamma * (all_L[\"transition\"] @ self.final_valuef_PI)\n",
    "        Q_star[:,1] = all_R[\"reward\"] + self.gamma * (all_R[\"transition\"] @ self.final_valuef_PI)\n",
    "        self.Q_star = Q_star\n",
    "        # Simulates the MDP under a fixed policy\n",
    "    def simulate_mdp(self, rounds = 10**5,  p_go_right=np.array([0.5,0.5,0.5,0.5])):\n",
    "        environemnts = []\n",
    "        self.reset_all()\n",
    "        # Init simulation in left_most state\n",
    "        for i in range(rounds):\n",
    "            if i == 0:\n",
    "                index = 0\n",
    "            action_sample = np.random.choice([\"L\",\"R\"], size = 1, p = [1-p_go_right[index], p_go_right[index]])[0]\n",
    "            next,reward = self.nodes[index].visit(action_sample)\n",
    "            index += next\n",
    "            empirical_nodes = [self.nodes[j].get_estimate() for j in range(self.n_states)]\n",
    "            environemnts.append(RiverSwimEnvironment(gamma=self.gamma, nodes = empirical_nodes))\n",
    "        self.emperical_environments = environemnts\n",
    "\n",
    "    def q_learning(self, alpha = \"constant\", incremental = False ,rounds = 10**5, p_go_right=np.array([0.5,0.5,0.5,0.5])):\n",
    "        # Initialize q and reset all nodes\n",
    "        self.reset_all()\n",
    "        q_t = np.zeros((rounds, self.n_states, 2))\n",
    "\n",
    "        #Convience translator between left and right and 0/1 for referencing numpy arrays\n",
    "        action_dict = {\"L\":0,\"R\":1}\n",
    "        reverse_dict = {\"0\":\"L\",\"1\":\"R\"}\n",
    "\n",
    "        # Specify learning rates\n",
    "        if alpha == \"constant\":\n",
    "            learning_rate = np.arange(rounds)+1\n",
    "            learning_rate = 2/(learning_rate**(2/3)+1)\n",
    "        else:\n",
    "            learning_rate = np.zeros(rounds)\n",
    "\n",
    "        # Specifies whether learning should be inceremental, and if so, sets B_t\n",
    "        if incremental:\n",
    "            B_t = np.zeros((rounds, self.n_states, 2))\n",
    "\n",
    "        for i in range(rounds):\n",
    "            if i == 0:\n",
    "                index = 0\n",
    "            \n",
    "            # Updates all q-values (and also B-values) to that of the previous round - done here because of index convinience\n",
    "            q_t[i,:,:] = q_t[i-1,:,:]\n",
    "            if incremental:\n",
    "                B_t[i,:,:] = B_t[i-1,:,:]\n",
    "            \n",
    "            # If not incremental sample randomly according to prescribed policy\n",
    "            if not incremental:\n",
    "                action_sample = np.random.choice([\"L\",\"R\"], size = 1, p = [1-p_go_right[index], p_go_right[index]])[0]\n",
    "            # If incremental, take the greedy action w.r.t. Q + B\n",
    "            if incremental:\n",
    "                action_sample = np.argmax(q_t[i,index,:]+B_t[i,index,:])\n",
    "                action_sample = reverse_dict[str(action_sample)]\n",
    "            # If learning is adpative update step-size parameter\n",
    "            if alpha == \"adaptive\":\n",
    "                learning_rate[i] = 2/(self.nodes[index].actions_taken[action_sample]**(2/3)+1)\n",
    "\n",
    "\n",
    "            next,reward = self.nodes[index].visit(action_sample)\n",
    "\n",
    "            delta = reward + self.gamma*np.max(q_t[i-1,index+next,:]) - q_t[i-1,index,action_dict[action_sample]]\n",
    "\n",
    "            q_t[i,index,action_dict[action_sample]] += learning_rate[i]*delta\n",
    "            \n",
    "            if incremental:\n",
    "            # Update B_t \n",
    "                for action in [\"L\",\"R\"]:\n",
    "                    if action == action_sample:\n",
    "                        B_t[i,index,action_dict[action]] = 0\n",
    "                    else:\n",
    "                        B_t[i,index,action_dict[action]] += 1/self.nodes[index].times_since_last_play[action]\n",
    "            index += next\n",
    "        return q_t\n",
    "ergodic_4 = RiverSwimEnvironment(gamma = 0.95, n_states = 4, nodes = None,reward_leftbank ={\"L\":0.05, \"R\":0}, reward_other = {\"L\":0, \"R\":0}, reward_rightbank = {\"L\":0, \"R\":1},\n",
    "    trans_leftbank = {\"L\":np.array([0,0.95,0.05]),\"R\":np.array([0,0.6,0.4])}, trans_other = {\"L\":np.array([0.95,0,0.05]),\"R\":np.array([0.05,0.55,0.4])},\n",
    "    trans_rightbank = {\"L\":np.array([1,0,0]),\"R\":np.array([0.4,0.6,0])})\n",
    "ergodic_4.gen_all()\n",
    "ergodic_4.policy_iteration()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
